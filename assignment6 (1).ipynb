{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "n=4\n",
    "actual=[1.0,1.5,2.8,3.7]\n",
    "pred=[1.1, 1.3,3.2,3.7]\n",
    "import numpy as np\n",
    "\n",
    "def mse(actual, pred): \n",
    "    actual, pred = np.array(actual), np.array(pred)\n",
    "    return np.square(np.subtract(actual,pred)).mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05250000000000007"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(actual, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTSC670: Foundations of Machine Learning Models\n",
    "\n",
    "## Assignment 6: Classification System Metrics\n",
    "\n",
    "#### Name:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CodeGrade\n",
    "\n",
    "Note that this assignment will be automatically graded through CodeGrade and you will have unlimited submission attempts.  When submitting to CodeGrade, your notebook should be named `assignment6.ipynb` and there should be no errors in the file or CodeGrade will not be able to grade it.  Before submitting, I suggest that you restart your kernel and attempt to run all cells again to ensure that there will be no errors when CodeGrade runs your script.\n",
    "\n",
    "It is very important that all written functions have the function parameters in the same order as given to you in the respective instructions.  \n",
    "\n",
    "Do not use the built-in Scikit-Learn functions when creating your functions from scratch.  Instead, you may use those functions after to verify your calculations.  Your assignments will be checked and points will be manually taken off if you use Scikit-Learn functions in your created functions.\n",
    "\n",
    "<u style=\"color:red;\">**Important: Do not round any of your outputs or CodeGrade will count them as incorrect**</u>\n",
    "\n",
    "\n",
    "## Assignment Details\n",
    "\n",
    "The purpose of this assignment is to familiarize you with the metrics used to measure prediction performance in classification systems.  Suppose there 20 binary observations whose target values are:\n",
    "\n",
    "$$[1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1]$$\n",
    "\n",
    "Suppose that your machine learning model returns prediction probabilities ([predict_proba()](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba) in sklearn) of:\n",
    "\n",
    "$$[0.886, 0.375, 0.174, 0.817, 0.574, 0.319, 0.812, 0.314, 0.098, 0.741, 0.847, 0.202, 0.31 , 0.073, 0.179, 0.917, 0.64 , 0.388, 0.116, 0.72]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Model Predictions\n",
    "\n",
    "Begin by writing a function from scratch called `predict()` that accepts as input the following (in this exact order):\n",
    "- a list of prediction probabilities (as a list)\n",
    "- threshold value (as a float)\n",
    "\n",
    "This function should compute the final predictions to be output by the model and return them as a list.  If a prediction probability value is less than or equal to the threshold value, then the prediction is the negative case (i.e. 0).  If a prediction probability value is greater than the threshold value, then the prediction is the positive case (i.e. 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(probs, thresh):\n",
    "    pred = []  \n",
    "    for i in range(len(probs)):\n",
    "        if probs[i] >= thresh: \n",
    "            pred.append(1) \n",
    "        else:\n",
    "            pred.append(0)\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a list of prediction probabilities (as given in the Assignment Details section) called `probs` and a variable called `thresh` that has the value 0.5.  Then invoke the `predict()` function to calculate the model predictions using those variables.  Save this output as `preds` and print it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction probabilities\n",
    "probs = [0.886,0.375,0.174,0.817,0.574,0.319,0.812,0.314,0.098,0.741,\n",
    "         0.847,0.202,0.31,0.073,0.179,0.917,0.64,0.388,0.116,0.72]\n",
    "\n",
    "# threshold value\n",
    "thresh = 0.5\n",
    "\n",
    "# prediction values\n",
    "preds = predict(probs, thresh)\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Model Accuracy\n",
    "\n",
    "Write a function from scratch called `acc_score()` that accepts as input (in this exact order):\n",
    "- a list of true labels \n",
    "- a list of model predictions\n",
    "\n",
    "This function should calculate the model accuracy score using the true labels as compared to the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_score(labels, preds):\n",
    "    correctly_predicted = 0\n",
    "    # iterating over every label and checking it with the true sample  \n",
    "    for true_label, predicted in zip(labels, preds):  \n",
    "        if true_label == predicted:  \n",
    "            correctly_predicted += 1  \n",
    "    accuracy_score = correctly_predicted / len(labels)  \n",
    "    return accuracy_score  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the accuracy score using your function `acc_score()`, and pass as input the true labels (listed below as `labels`) and the model predictions you calculated above (`preds`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy:  0.85\n"
     ]
    }
   ],
   "source": [
    "# true labels\n",
    "labels = [1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1]\n",
    "\n",
    "accuracy = acc_score(labels, preds)\n",
    "print(\"Model Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** Use the Scikit-Learn's [accuracy_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function to check that the value you computed using `acc_score()` is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.85\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(labels, preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Model Error Rate\n",
    "\n",
    "Write a function from scratch called `error_rate()` that accepts as input (in this exact order):\n",
    "- a list of true labels\n",
    "- a list of model predictions\n",
    "\n",
    "This function should calculate the model error rate and should use your `acc_score()` function that you previously defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(labels, preds):\n",
    "    error=1-acc_score(labels, preds)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the model error rate for the true labels and the model predictions previously given.  Name the error rate that you calculate `error` in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Error Rate:  0.15000000000000002\n"
     ]
    }
   ],
   "source": [
    "error =error_rate(labels, preds)\n",
    "print(\"Model Error Rate: \", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the Model Precision and Recall\n",
    "\n",
    "Write a function from scratch called `prec_recall_score()` that accepts as input (in this exact order):\n",
    "- a list of true labels \n",
    "- a list of model predictions\n",
    "\n",
    "This function should compute and return _both_ the model precision and recall (in that order).  \n",
    "\n",
    "Do not use the built-in Scikit-Learn functions `precision_score()`,`recall_score()`, `confusion_matrix()`, or Panda's `crosstab()` to do this.  Instead, you may use those functions after to verify your calculations. We want to ensure that you understand what is going on behind-the-scenes of the precision and recall functions by creating similar ones from scratch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prec_recall_score(labels, preds):\n",
    "    correctly_predicted = 0\n",
    "    # iterating over every label and checking it with the true sample  \n",
    "    for true_label, predicted in zip(labels, preds):  \n",
    "        if true_label == predicted:  \n",
    "            correctly_predicted += 1  \n",
    "    accuracy_score = correctly_predicted / len(labels)  \n",
    "    return accuracy_score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use your `prec_recall_score` function to compute `precision` and `recall` for the true labels and the model predictions you calculated previously.  Save your output as `precision` and `recall`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall = ### ENTER CODE HERE ###\n",
    "print(\"Precision = \", precision)\n",
    "print(\"Recall = \", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** Use Scikit-Learn's `precision_score()` and `recall_score()` to verify that your calculations above are correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn Precision Score\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sklearn Recall Score\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate $F_\\beta$ Scores\n",
    "\n",
    "Write a function from scratch called `f_beta` that computes the $F_\\beta$ measure for any value of $\\beta$.  \n",
    "\n",
    "- This function must invoke the `prec_recall_score` function you wrote above in order to obtain the values for precision and recall.  \n",
    "- The function must take as input (in this exact order):\n",
    "    - a list of true labels\n",
    "    - a list of model predictions you calculated previously\n",
    "    - the value of $\\beta$ you wish to use in the calculation \n",
    "    \n",
    "We defined $F_\\beta$ in class to be:\n",
    "\n",
    "$$ F_\\beta = \\frac{(\\beta^2+1) \\cdot Pr \\cdot Re}{\\beta^2 \\cdot Pr + Re} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use your `f_beta` function to compute the $F_1$ score for the true labels and the model predictions you calculated previously.  Save your output as `F1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F1 = ### ENTER CODE HERE ###\n",
    "print(\"F1 = \", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** Verify your above calculation is correct by invoking Scikit-Learn's `f1_score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the TPR and FPR for ROC Curve\n",
    "\n",
    "In the subsequent cells, you will be asked to plot an ROC curve.  The ROC curve plots the True Positive Rate (TPR, also called recall) against the False Positive Rate (FPR).  Both of these are scalar values, akin to precision and recall.\n",
    "\n",
    "Write a function from scratch called `TPR_FPR_score` that is nearly identical to `prec_recall_score` that you wrote previously, which computes and returns TPR and FPR (in that order).  The function must take as input (in this exact order):\n",
    "- a list of true labels \n",
    "- a list of model predictions you calculated previously\n",
    "\n",
    "TPR and FPR are defined as follows:\n",
    "\n",
    "$$ TPR = recall = \\frac{TP}{TP + FN} $$\n",
    "\n",
    "$$ FPR = \\frac{FP}{FP + TN} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** Invoke the `TPR_FPR_score` function using your `labels` and `preds` from previous steps.  Your output should be the following:  `(0.875, 0.16666666666666666)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute and Plot the ROC Curve\n",
    "\n",
    "Write a function from scratch called `roc_curve_computer` that accepts (in this exact order):\n",
    "- a list of true labels\n",
    "- a list of prediction probabilities (notice these are probabilities and not predictions - you will need to obtain the predictions from these probabilities)\n",
    "- a list of threshold values.  \n",
    "\n",
    "The function must compute and return the True Positive Rate (TPR, also called recall) and the False Positive Rate (FPR) for each threshold value in the threshold value list that is passed to the function. \n",
    "\n",
    "**Important:** Be sure to reuse functions and code segments from your work above! You should reuse two of your above created functions so that you do not duplicate your code.  \n",
    "\n",
    "The function you will write behaves identically to Scikit-Learn's [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve) function, except that it will take the list of thresholds in as input rather than return them as output.  Your function must calculate one value of TPR and one value of FPR for each of the threshold values in the list.  \n",
    "\n",
    "Your function will output a list of TPR values and a list of FPR values (in that order).  You will then take these TPR and FPR values, and plot them against each other to create the [Receiver Operating Characteristic (ROC) curve](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html).\n",
    "\n",
    "You must not use any built-in library function to perform the calculation of a performance metric.  You may of course use common, built-in Python functions, such as: `range()`, `len()`, et cetera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** As an example, calling the `roc_curve_computer` function with the input `true_labels = [1, 0, 1, 0, 0]`, `pred_probs = [0.875, 0.325, 0.6, 0.09, 0.4]`, and `thresholds = [0.00, 0.25, 0.50, 0.75, 1.00]` yields the output:\n",
    "\n",
    "`TPR =  [1.0, 1.0, 1.0, 0.5, 0.0]` and `FPR =  [1.0, 0.6666, 0.0, 0.0, 0.0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = [1, 0, 1, 0, 0]\n",
    "pred_probs = [0.875, 0.325, 0.6, 0.09, 0.4]\n",
    "thresholds = [0.00, 0.25, 0.50, 0.75, 1.00]\n",
    "\n",
    "### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use your `roc_curve_computer` function along with the threshold values `thresholds = [x/100 for x in range(101)]` to compute the TPR and FPR lists for the provided data and save your output as `TPR` and `FPR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [x/100 for x in range(101)]\n",
    "TPR, FPR = roc_curve_computer(labels, probs, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following plotting function to plot the ROC curve.  Pass the TPR and FPR values that you calculated above into the plotting function to view the ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc_curve(tpr, fpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--') # dashed diagonal line\n",
    "    plt.title('Receiver Operating Characteristic', fontsize=12)\n",
    "    plt.axis([-0.015, 1.0, 0, 1.02])\n",
    "    plt.xlabel('False Positive Rate (Fall-Out)', fontsize=12)\n",
    "    plt.ylabel('True Positive Rate (Recall)', fontsize=12)\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plot_roc_curve(TPR, FPR)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Code Check:** Next, compare your plot to the plot generated by Scikit-Learn's `roc_curve` function.  Use Scikit-Learn's `roc_curve` function to calculate the false positive rates, the true positive rates, and the thresholds.  Save the output using sklearn's function as `fpr`, `tpr`, and `thresholds`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = ### ENTER CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pass the false positive rates and the true positive rates obtained above via the Scikit-Learn function as input to the `plot_roc_curve` function in order to compare ROC curves. These two plots should look the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plot_roc_curve(tpr, fpr)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
